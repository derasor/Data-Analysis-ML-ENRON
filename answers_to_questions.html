
<!-- saved from url=(0088)file:///C:/Users/David/Documents/GitHub/Data-Analysis-ML-ENRON/answers_to_questions.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><style type="text/css">ol.lst-kix_otxxp6dydhwn-4.start{counter-reset:lst-ctn-kix_otxxp6dydhwn-4 0}.lst-kix_otxxp6dydhwn-5>li{counter-increment:lst-ctn-kix_otxxp6dydhwn-5}.lst-kix_otxxp6dydhwn-8>li{counter-increment:lst-ctn-kix_otxxp6dydhwn-8}ol.lst-kix_otxxp6dydhwn-1.start{counter-reset:lst-ctn-kix_otxxp6dydhwn-1 0}.lst-kix_otxxp6dydhwn-6>li:before{content:"" counter(lst-ctn-kix_otxxp6dydhwn-0,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-1,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-2,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-3,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-4,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-5,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-6,decimal) ". "}ol.lst-kix_otxxp6dydhwn-6.start{counter-reset:lst-ctn-kix_otxxp6dydhwn-6 0}.lst-kix_otxxp6dydhwn-7>li:before{content:"" counter(lst-ctn-kix_otxxp6dydhwn-0,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-1,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-2,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-3,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-4,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-5,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-6,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-7,decimal) ". "}.lst-kix_otxxp6dydhwn-8>li:before{content:"" counter(lst-ctn-kix_otxxp6dydhwn-0,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-1,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-2,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-3,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-4,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-5,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-6,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-7,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-8,decimal) ". "}.lst-kix_otxxp6dydhwn-2>li{counter-increment:lst-ctn-kix_otxxp6dydhwn-2}.lst-kix_otxxp6dydhwn-0>li{counter-increment:lst-ctn-kix_otxxp6dydhwn-0}.lst-kix_otxxp6dydhwn-6>li{counter-increment:lst-ctn-kix_otxxp6dydhwn-6}ol.lst-kix_otxxp6dydhwn-3.start{counter-reset:lst-ctn-kix_otxxp6dydhwn-3 0}.lst-kix_otxxp6dydhwn-3>li{counter-increment:lst-ctn-kix_otxxp6dydhwn-3}.lst-kix_otxxp6dydhwn-5>li:before{content:"" counter(lst-ctn-kix_otxxp6dydhwn-0,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-1,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-2,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-3,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-4,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-5,decimal) ". "}.lst-kix_otxxp6dydhwn-4>li{counter-increment:lst-ctn-kix_otxxp6dydhwn-4}ol.lst-kix_otxxp6dydhwn-7.start{counter-reset:lst-ctn-kix_otxxp6dydhwn-7 0}.lst-kix_otxxp6dydhwn-4>li:before{content:"" counter(lst-ctn-kix_otxxp6dydhwn-0,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-1,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-2,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-3,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-4,decimal) ". "}ol.lst-kix_otxxp6dydhwn-8{list-style-type:none}ol.lst-kix_otxxp6dydhwn-7{list-style-type:none}ol.lst-kix_otxxp6dydhwn-8.start{counter-reset:lst-ctn-kix_otxxp6dydhwn-8 0}ol.lst-kix_otxxp6dydhwn-2.start{counter-reset:lst-ctn-kix_otxxp6dydhwn-2 0}ol.lst-kix_otxxp6dydhwn-6{list-style-type:none}.lst-kix_otxxp6dydhwn-1>li{counter-increment:lst-ctn-kix_otxxp6dydhwn-1}ol.lst-kix_otxxp6dydhwn-5{list-style-type:none}.lst-kix_otxxp6dydhwn-3>li:before{content:"" counter(lst-ctn-kix_otxxp6dydhwn-0,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-1,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-2,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-3,decimal) ". "}ol.lst-kix_otxxp6dydhwn-4{list-style-type:none}ol.lst-kix_otxxp6dydhwn-3{list-style-type:none}ol.lst-kix_otxxp6dydhwn-2{list-style-type:none}ol.lst-kix_otxxp6dydhwn-1{list-style-type:none}.lst-kix_otxxp6dydhwn-0>li:before{content:"" counter(lst-ctn-kix_otxxp6dydhwn-0,decimal) ". "}ol.lst-kix_otxxp6dydhwn-0{list-style-type:none}ol.lst-kix_otxxp6dydhwn-0.start{counter-reset:lst-ctn-kix_otxxp6dydhwn-0 0}.lst-kix_otxxp6dydhwn-2>li:before{content:"" counter(lst-ctn-kix_otxxp6dydhwn-0,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-1,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-2,decimal) ". "}.lst-kix_otxxp6dydhwn-1>li:before{content:"" counter(lst-ctn-kix_otxxp6dydhwn-0,decimal) "." counter(lst-ctn-kix_otxxp6dydhwn-1,decimal) ". "}ol.lst-kix_otxxp6dydhwn-5.start{counter-reset:lst-ctn-kix_otxxp6dydhwn-5 0}.lst-kix_otxxp6dydhwn-7>li{counter-increment:lst-ctn-kix_otxxp6dydhwn-7}ol{margin:0;padding:0}.c0{line-height:1.15;orphans:2;widows:2;direction:ltr;height:11pt}.c1{line-height:1.15;orphans:2;widows:2;direction:ltr}.c8{orphans:2;widows:2;direction:ltr;height:11pt}.c6{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c3{padding:0;margin:0}.c4{margin-left:36pt;padding-left:0pt}.c5{color:#1155cc;text-decoration:underline}.c7{color:inherit;text-decoration:inherit}.c2{font-weight:bold}.title{padding-top:0pt;color:#000000;font-size:21pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:13pt;padding-bottom:10pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:10pt;color:#000000;font-size:16pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:10pt;color:#000000;font-weight:bold;font-size:13pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:8pt;color:#666666;font-weight:bold;font-size:12pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:8pt;color:#666666;text-decoration:underline;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:8pt;color:#666666;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:8pt;color:#666666;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c6">
<p class="c1"><span class="c2">UDACITY Data Analyst Nanodegree</span></p>
<p class="c1"><span class="c2">Intro to Machine Learning - Identifying Fraud from Enron Email</span></p>
<p class="c1"><span class="c2">David Eraso</span></p>
<p class="c1"><span class="c2">-------------------------------</span></p><p class="c1"><span class="c2">Enron Submission Free-Response Questions</span></p><p class="c0"><span></span></p><p class="c0"><span></span></p><p class="c1"><span>REFERENCES<br><br> 
Remove outliers:<br><br> 
[1] Udacity - Intro To Machine Learning, Lesson 7--Outliers<br><br>
Create new features:<br><br>
[2] Udacity - Intro To Machine Learning, Lesson 11--Feature Selection<br><br>
SelectKBest:<br><br>
[3] </span><span class="c5"><a class="c7" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html"></a>http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html</span><span>&nbsp;<br><br>
[4] </span><span class="c5"><a class="c7" href="http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection"></a>http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection</span><span>&nbsp;<br><br>
[5] </span><span class="c5"><a class="c7" href="http://stackoverflow.com/questions/25792012/feature-selection-using-scikit-learn"></a>http://stackoverflow.com/questions/25792012/feature-selection-using-scikit-learn</span><span>&nbsp;<br><br>
[6] </span><span class="c5"><a class="c7" href="http://stackoverflow.com/questions/21471513/sklearn-selectkbest-which-variables-were-chosen"></a>http://stackoverflow.com/questions/21471513/sklearn-selectkbest-which-variables-were-chosen</span><span>&nbsp;<br><br>
MinMaxScaler:<br><br>
[7] </span><span class="c5"><a class="c7" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"></a>http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html</span><span>&nbsp;<br><br>
Principal Component Analysis:<br><br>
[8] </span><span class="c5"><a class="c7" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"></a>http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</span><span>&nbsp;<br><br>
[9] </span><span class="c5"><a class="c7" href="http://stackoverflow.com/questions/22984335/recovering-features-names-of-explained-variance-ration-in-pca-with-sklearn"></a>http://stackoverflow.com/questions/22984335/recovering-features-names-of-explained-variance-ration-in-pca-with-sklearn</span><span>&nbsp;<br><br>
GridSearchCV:<br><br>
[10] </span><span class="c5"><a class="c7" href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html"></a>http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html</span><span>&nbsp;<br><br>
Pipeline:<br><br>
[11] </span><span class="c5"><a class="c7" href="http://scikit-learn.org/stable/modules/pipeline.html"></a>http://scikit-learn.org/stable/modules/pipeline.html</span><span>&nbsp;<br><br>
k-fold cross-validation:<br><br>
[12] </span><span class="c5"><a class="c7" href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html"></a>http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html</span><span>&nbsp;<br><br>
[13] </span><span class="c5"><a class="c7" href="http://www.analyticsvidhya.com/blog/2015/05/k-fold-cross-validation-simple/"></a>http://www.analyticsvidhya.com/blog/2015/05/k-fold-cross-validation-simple/</span><span>&nbsp;<br><br>
[14] </span><span class="c5"><a class="c7" href="http://stackoverflow.com/questions/24215886/typeerror-when-attempting-cross-validation-in-sklearn"></a>http://stackoverflow.com/questions/24215886/typeerror-when-attempting-cross-validation-in-sklearn</span><span>&nbsp;<br><br>
Other:<br><br>
[15] </span><span class="c5"><a class="c7" href="http://stackoverflow.com/questions/2142453/getting-list-without-kth-element-efficiently-and-non-destructively"></a>http://stackoverflow.com/questions/2142453/getting-list-without-kth-element-efficiently-and-non-destructively</span><span>&nbsp;<br><br>
[16] </span><span class="c5"><a class="c7" href="http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.asarray.html"></a>http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.asarray.html</span><span>&nbsp;<br><br>
</span></p><p class="c0"><span></span></p><p class="c0"><span></span></p><ol class="c3 lst-kix_otxxp6dydhwn-0 start" start="1"><li class="c1 c4"><span>Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those? &nbsp;[relevant rubric items: “data exploration”, “outlier investigation”]</span></li></ol><p class="c0"><span></span></p>

<p class="c1"><span>The goal of this project is to use emails and financial data from people involved with ENRON, with Machine Learning techniques, to deploy a Person of Interest Identifier, that identifies persons who played an active part in the ENRON scandal. The ENRON scandal is a very notorious White Collar Crime case and huge audit failure that led to massive destruction of capital. The idea is to train a classifier with the labeling available in the dataset to produce a reliable classification of eventual new data points.</span></p>&nbsp;
<p class="c1"><span>The ENRON dataset used in this project is a mix of email and financial features, for a total of 21, in the form of a pickled dict of dicts. Each item has the following form:
</span></p><xmp>('LASTNAME FIRSTNAME MIDDLEINITIAL', {  'salary': int or NaN, 
					'to_messages': int or NaN,
					'deferral_payments': int or NaN, 
					'total_payments': int or NaN, 
					'exercised_stock_options': int or NaN,
					'bonus': int or NaN, 
					'restricted_stock': int or NaN, 
					'shared_receipt_with_poi': int or NaN, 
					'restricted_stock_deferred': int or NaN, 
					'total_stock_value': int or NaN, 
					'expenses': int or NaN, 
					'loan_advances': int or NaN, 
					'from_messages': int or NaN, 
					'other': int or NaN, 
					'from_this_person_to_poi': int or NaN, 
					'poi': boolean, 
					'director_fees': int or NaN, 
					'deferred_income': int or NaN, 
					'long_term_incentive': int or NaN, 
					'email_address': str or NaN, 
					'from_poi_to_this_person': int or NaN})</xmp>

  <p class="c1"><span>There are 146 data points. Only 18 entries are classified as Person Of Interest (POI), that is 8.1% of the total dataset. The remaining 128 are non-POI. This is the only feature that has not missing values. The proportin of NaN for each feature is as follows:</span></p><xmp>  'salary': 0.35172413793103446, 
  'to_messages': 0.4068965517241379, 
  'deferral_payments': 0.7379310344827587, 
  'total_payments': 0.14482758620689656, 
  'loan_advances': 0.9793103448275862, 
  'bonus': 0.4413793103448276, 
  'email_address': 0.23448275862068965, 
  'restricted_stock_deferred': 0.8827586206896552, 
  'total_stock_value': 0.13793103448275862, 
  'shared_receipt_with_poi': 0.4068965517241379, 
  'long_term_incentive': 0.5517241379310345, 
  'exercised_stock_options': 0.30344827586206896, 
  'from_messages': 0.4068965517241379, 
  'other': 0.36551724137931035, 
  'from_poi_to_this_person': 0.4068965517241379, 
  'from_this_person_to_poi': 0.4068965517241379, 
  'poi': 0.0, 
  'deferred_income': 0.6689655172413793, 
  'expenses': 0.35172413793103446, 
  'restricted_stock': 0.2482758620689655, 
  'director_fees': 0.8896551724137931</xmp><p class="c1"><span>Particularly high values are for 'loan advances' (97.9%), 'director fees' (89%),'restricted stock deferred' (88.2%), 'deferral payments' (73.8%), and 'deferred income' (66.9%).</span></p>
&nbsp;<p class="c1"><span>Also, as implemented in Lesson 7 through plotting salary vs. bonus, there is a clear outlier in the dataset that corresponds to "TOTAL" values. This is does not represent any person, but the total values for the financial data, and thus this entry is removed from the dataset for further analysis. There are other outliers but they are valid data points that represent the most prominent figures of the scandal such as Kenneth Lay and Jeffrey Skilling.</span></p>


<p class="c0"><span></span></p><p class="c0"><span></span></p><ol class="c3 lst-kix_otxxp6dydhwn-0" start="2"><li class="c1 c4"><span>What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values. &nbsp;[relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]</span></li></ol><p class="c0"><span></span></p>

<p class="c1"><span>The variable that I am trying to predict is 'poi', this is the label. In order to choose the features of my model, and taking into account the previous analysis, The first thing I did was to keep features with low proportion of NaN values as a first filter. Such features include 'total stock value', 'total payments', 'restricted stock', 'exercised stock options', 'expenses', and 'salary'. 'email address' has a relatively low prportion of NaN but does not seem to have a direct relevance to de label since the goal is precisely to identify POIs regardless of their name, and 'email address' is a proxy to this prior identity. Also it is worth noting that the variables choosed in this first filter are all from the financial part of the dataset. To complement this set of features I added four features from the email part of the dataset that interestingly have the same proportion of NaN entries; 'to messages', 'from messages', 'from poi to this person', and 'from this person to poi'.<br><br>
My first exploration of feature importance was to actually deploy a Decision Tree classifier and check feature importances. From this first approach I have 'exercised_stock_options', 'expenses' and 'restricted_stock' as possible candidates. Also, in the email features, one of 'from_poi_to_this_person' and 'from_this_person_to_poi' may be important. However, regarding these last features, as discussed in lesson 11, a more relevant form would be to have a proportion o e-mails from/to poi. This is in a way a custom scaling, and so the new features 'fraction_to_poi' and 'fraction_from_poi' are created.
<br><br>
Since the dataset is not particularly large, and the labeling is skewed toward a large proportion of non-POIs, I keep the number of features low, at four. Even though I will not use PCA in the final implementation since the principal components will not be any of the original features, but a linear combination of them, I am curious to explore Principal Component Analysis with a set of features that take into account all of the features in the first set, plus the new created features to see if I can gain any further insight with the results. PCA needs scaling, and thus it is implemented for this part. In the results 'exercised_stock_options' is important in the second and third PC, but in neither case it is the main contributor. 'expenses' is the main contributor in PC(4) and very important in PC(3) along with 'fraction_to_poi', this last feature it's also very important in PC(2) with 'fraction_from_poi'. But in general I don't feel that these results give me a decisive insight, so I will keep with the features chosen by SelectKBest: 'exercised_stock_options', 'expenses', 'fraction_from_poi', and 'restricted_stock'.
</span></p>

<p class="c0"><span></span></p><p class="c0"><span></span></p><ol class="c3 lst-kix_otxxp6dydhwn-0" start="3"><li class="c1 c4"><span>What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms? &nbsp;[relevant rubric item: “pick an algorithm”]</span></li></ol><p class="c0"><span></span></p>
<p class="c1"><span>I tried a variety of classifiers training on the whole dataset. I recorded performance for each of them using the output from the provided code 'tester.py' in order to compare and decide what algorithm I ended up using in my classifier.
<br><br>
The starting point was a Gaussian Naive Bayes classifier, in the ouput of 'tester.py', I notice a large number of predictions (14000) compared to the size of my dataset (145). The reason, as explained in Task 5 description, is the use of stratified shuffle split cross validation. Also, for this initial classifier, there is a large number of false negatives, and accordingly, a bad Recall. I tried then a Decision Tree using GridSearchCV for systematic tuning of parameters, specifically 'criterion' and 'min_samples_split'. Accuracy and precision were slightly lower,   but Recall improved. However, it seemed, overall, a more balanced classifier. A very important characteristic due to the skewness of labeling. I walso tried AdaBoost with Decision Tree Classifier as weak learner; but, this ensemble method gave worst results. Finally I tried SVC with scaling (since I tested rbf kernel). In this case I used again selectKBest for feature selection using the feature set with the initial 10 features plus the 2 new features. For this test I used Pipeline and Gridsearch. Again, recall did very poorly since there were a large number for false negatives just like with the initial Gaussian Naive Bayes classifier.
<br><br>
Since the Decision Tree Classifier gave the most balanced results, I chose to use this algorithm.
</span></p>

<p class="c0"><span></span></p><p class="c0"><span></span></p><ol class="c3 lst-kix_otxxp6dydhwn-0" start="4"><li class="c1 c4"><span>What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well? &nbsp;How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier). &nbsp;[relevant rubric item: “tune the algorithm”]</span></li></ol><p class="c0"><span></span></p>
<p class="c1"><span>Most classification algorithms have parameters that define the inner process of the training. Depending on the problem you are tackling, and the nature of the dataset, different combinations of values and specifications for these parameters can yiled different outcomes in the performance of the classifier. In this case, a Desission Tree classifier was used. This classifier, as defined in the sickit-learn library, gives the possibility of inspecting features importances, as used in feature selection. Also, as mentioned before, GridsearchCV was used to automatically tune the parameters 'criterion' (The function to measure the quality of a split. “gini” for the Gini impurity, and “entropy” for the information gain)[], and 'min_samples_split' (The minimum number of samples required to split an internal node)[].
</span></p>

<p class="c0"><span></span></p><p class="c0"><span></span></p><ol class="c3 lst-kix_otxxp6dydhwn-0" start="5"><li class="c1 c4"><span>What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis? &nbsp;[relevant rubric item: “validation strategy”]</span></li></ol><p class="c0"><span></span></p><p class="c1"><span>In validation you try to evaluate the predictive power of your classifier. For this, it is necessary to split your dataset into two subsets, a train set, and a test set. Usually you want your train set to be large enough so that the training can be almost as effective as if you were training with the whole dataset, then you ask your classifier to classify the data points of your test set and since you already know the correct labeling, you can assess, among other things, the accuracy of your classifier. One classic mistake in validating your model could be to overfit your classifier with the available dataset, so that accuracy is really high, but classification of new entries is poor. 
<br><br>
Given the nature of this dataset, splitting randomly does not guarantee a training subset that effectively recognizes the differences between the two labels since there are so many more instances of non-POIs compared to instances of POIs. This is why I used k-fold cross-validation to split the dataset into train and test sets, and chose the splitting that best trained my classifier.</span></p>

<p class="c0"><span></span></p><p class="c0"><span></span></p><ol class="c3 lst-kix_otxxp6dydhwn-0" start="6"><li class="c1 c4"><span>Give at least 2 evaluation metrics and your average performance for each of them. &nbsp;Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]</span></li></ol><p class="c0"><span></span></p>
<p class="c1"><span>The most decisive aspect in the process was using k-fold cross validation to choose the best train-test splitting. Using this tool I was able to achieve an accuracy of 0.913043478261, a recall of 0.5, and a precision of 0.75. When testing different algorithms, a recurrent concern was the high occurences of false negatives, and thus a low recall. Having false negatives is equivalent of not identifying Persons Of Interest, when they actually are, so improving this metric was important. In the end precision is higher than recall, having high precision in this context is equivalent to having low false positives, or in other words, having a relative lower probability of classifying someone as POI erroneously. Lowering this probability is maybe even more important, so the fact that precision is even higher is very desirable.
</span></p>
<p class="c8"><span></span></p></body></html>